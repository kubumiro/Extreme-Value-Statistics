{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"odule containing classical generalistic models\n",
    "Gumbel:\n",
    "    To be used applying the Block Maxima approach\n",
    "    \n",
    "Generalised extreme value distribution (GEV):\n",
    "    To be used applying the Block Maxima approach\n",
    "    \n",
    "Generalised Pareto Distribution (GPD):\n",
    "    To be used applying the Peak-Over-Threshold approach\n",
    "    TODO\n",
    "\"\"\"\n",
    "\n",
    "import warnings as _warnings\n",
    "\n",
    "from numpy.random import randint as _randint\n",
    "import numpy as _np\n",
    "import scipy.stats as _st\n",
    "from scipy import optimize as _op\n",
    "from scipy.special import gamma as _gamma\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Bootstrap confidence intervals calculations using percentile interval method\n",
    "###############################################################################\n",
    "class InstabilityWarning(UserWarning):\n",
    "    \"\"\"Issued when results may be unstable.\"\"\"\n",
    "    pass\n",
    "\n",
    "# On import, make sure that InstabilityWarnings are not filtered out.\n",
    "_warnings.simplefilter('always', InstabilityWarning)\n",
    "_warnings.simplefilter('always', UserWarning)\n",
    "\n",
    "def bootstrap_ci(data, statfunction=_np.average, alpha = 0.05, \n",
    "                 n_samples = 100):\n",
    "    \"\"\"\n",
    "    Given a set of data ``data``, and a statistics function ``statfunction`` that\n",
    "    applies to that data, computes the bootstrap confidence interval for\n",
    "    ``statfunction`` on that data. Data points are assumed to be delineated by\n",
    "    axis 0.\n",
    "    \n",
    "    This function has been derived and simplified from scikits-bootstrap \n",
    "    package created by cgevans (https://github.com/cgevans/scikits-bootstrap).\n",
    "    All the credits shall go to him.\n",
    "    **Parameters**\n",
    "    \n",
    "    data : array_like, shape (N, ...) OR tuple of array_like all with shape (N, ...)\n",
    "        Input data. Data points are assumed to be delineated by axis 0. Beyond this,\n",
    "        the shape doesn't matter, so long as ``statfunction`` can be applied to the\n",
    "        array. If a tuple of array_likes is passed, then samples from each array (along\n",
    "        axis 0) are passed in order as separate parameters to the statfunction. The\n",
    "        type of data (single array or tuple of arrays) can be explicitly specified\n",
    "        by the multi parameter.\n",
    "    statfunction : function (data, weights = (weights, optional)) -> value\n",
    "        This function should accept samples of data from ``data``. It is applied\n",
    "        to these samples individually. \n",
    "    alpha : float, optional\n",
    "        The percentiles to use for the confidence interval (default=0.05). The \n",
    "        returned values are (alpha/2, 1-alpha/2) percentile confidence\n",
    "        intervals. \n",
    "    n_samples : int or float, optional\n",
    "        The number of bootstrap samples to use (default=100)\n",
    "        \n",
    "    **Returns**\n",
    "    \n",
    "    confidences : tuple of floats\n",
    "        The confidence percentiles specified by alpha\n",
    "    **Calculation Methods**\n",
    "    \n",
    "    'pi' : Percentile Interval (Efron 13.3)\n",
    "        The percentile interval method simply returns the 100*alphath bootstrap\n",
    "        sample's values for the statistic. This is an extremely simple method of \n",
    "        confidence interval calculation. However, it has several disadvantages \n",
    "        compared to the bias-corrected accelerated method.\n",
    "        \n",
    "        If you want to use more complex calculation methods, please, see\n",
    "        `scikits-bootstrap package \n",
    "        <https://github.com/cgevans/scikits-bootstrap>`_.\n",
    "    **References**\n",
    "    \n",
    "        Efron (1993): 'An Introduction to the Bootstrap', Chapman & Hall.\n",
    "    \"\"\"\n",
    "\n",
    "    def bootstrap_indexes(data, n_samples=10000):\n",
    "        \"\"\"\n",
    "    Given data points data, where axis 0 is considered to delineate points, return\n",
    "    an generator for sets of bootstrap indexes. This can be used as a list\n",
    "    of bootstrap indexes (with list(bootstrap_indexes(data))) as well.\n",
    "        \"\"\"\n",
    "        for _ in range(n_samples):\n",
    "            yield _randint(data.shape[0], size=(data.shape[0],))    \n",
    "    \n",
    "    alphas = _np.array([alpha / 2,1 - alpha / 2])\n",
    "\n",
    "    data = _np.array(data)\n",
    "    tdata = (data,)\n",
    "    \n",
    "    # We don't need to generate actual samples; that would take more memory.\n",
    "    # Instead, we can generate just the indexes, and then apply the statfun\n",
    "    # to those indexes.\n",
    "    bootindexes = bootstrap_indexes(tdata[0], n_samples)\n",
    "    stat = _np.array([statfunction(*(x[indexes] for x in tdata)) for indexes in bootindexes])\n",
    "    stat.sort(axis=0)\n",
    "\n",
    "    # Percentile Interval Method\n",
    "    avals = alphas\n",
    "\n",
    "    nvals = _np.round((n_samples - 1)*avals).astype('int')\n",
    "\n",
    "    if _np.any(nvals == 0) or _np.any(nvals == n_samples - 1):\n",
    "        _warnings.warn(\"Some values used extremal samples; results are probably unstable.\", InstabilityWarning)\n",
    "    elif _np.any(nvals<10) or _np.any(nvals>=n_samples-10):\n",
    "        _warnings.warn(\"Some values used top 10 low/high samples; results may be unstable.\", InstabilityWarning)\n",
    "\n",
    "    if nvals.ndim == 1:\n",
    "        # All nvals are the same. Simple broadcasting\n",
    "        return stat[nvals]\n",
    "    else:\n",
    "        # Nvals are different for each data point. Not simple broadcasting.\n",
    "        # Each set of nvals along axis 0 corresponds to the data at the same\n",
    "        # point in other axes.\n",
    "        return stat[(nvals, _np.indices(nvals.shape)[1:].squeeze())]\n",
    "        \n",
    "###############################################################################\n",
    "# Function to estimate parameters of GEV using method of moments\n",
    "###############################################################################\n",
    "def gev_momfit(data):\n",
    "    \"\"\"\n",
    "    Estimate parameters of Generalised Extreme Value distribution using the \n",
    "    method of moments. The methodology has been extracted from appendix A.4\n",
    "    on EVA (see references below).\n",
    "    \n",
    "    **Parameters**\n",
    "    \n",
    "    data : array_like\n",
    "        Sample extreme data\n",
    "    \n",
    "    **Returns**\n",
    "    \n",
    "    tuple\n",
    "        tuple with the shape, location and scale parameters. In this,\n",
    "        case, the shape parameter is always 0.\n",
    "    \n",
    "    **References**\n",
    "    \n",
    "        DHI, (2003): '`EVA(Extreme Value Analysis - Reference manual) \n",
    "        <http://www.tnmckc.org/upload/document/wup/1/1.3/Manuals/MIKE%2011/eva/EVA_RefManual.pdf>`_', \n",
    "        DHI.\n",
    "    \"\"\"\n",
    "            \n",
    "    g = lambda n, x : _gamma(1 + n * x)\n",
    "    \n",
    "    mean = _np.mean(data)\n",
    "    std = _np.std(data)\n",
    "    skew = _st.skew(data)\n",
    "    \n",
    "    def minimize_skew(x):\n",
    "        a = -g(3, x) + 3 * g(1, x) * g(2, x) - 2 * g(1, x)**3\n",
    "        b = (g(2, x) - (g(1, x))**2)**1.5\n",
    "        c = abs(a / b - skew)\n",
    "        return c\n",
    "        \n",
    "    c = _op.fmin(minimize_skew, 0)[0] # first guess is set to 0\n",
    "    scale = std * abs(c) / _np.sqrt((g(2, c) - g(1, c)**2))\n",
    "    loc = mean - scale * (1 - g(1, c)) / c\n",
    "    \n",
    "    return c, loc, scale\n",
    "\n",
    "###############################################################################\n",
    "# Function to estimate parameters of Gumbel using method of moments\n",
    "###############################################################################\n",
    "def gum_momfit(data):\n",
    "    \"\"\"\n",
    "    Estimate parameters of Gumbel distribution using the \n",
    "    method of moments. The methodology has been extracted from Wilks \n",
    "    (see references below).\n",
    "    \n",
    "    **Parameters**\n",
    "    \n",
    "    data : array_like\n",
    "        Sample extreme data\n",
    "    \n",
    "    **Returns**\n",
    "    \n",
    "    tuple\n",
    "        tuple with the shape, location and scale parameters. In this,\n",
    "        case, the shape parameter is always 0.\n",
    "        \n",
    "    **References**\n",
    "    \n",
    "    \n",
    "        Wilks,D.S. (2006): '`Statistical Methods in the Atmospheric Sciences, \n",
    "        second edition <http://store.elsevier.com/Statistical-Methods-in-the-Atmospheric-Sciences/Daniel-Wilks/isbn-9780080456225/>`_', \n",
    "        Academic Press.\n",
    "    \"\"\"\n",
    "    \n",
    "    mean = _np.mean(data)\n",
    "    std = _np.std(data)\n",
    "    \n",
    "    euler_cte = 0.5772156649015328606065120900824024310421\n",
    "    \n",
    "    scale = std * _np.sqrt(6) / _np.pi    \n",
    "    loc = mean - scale * euler_cte\n",
    "    \n",
    "    return 0, loc, scale\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from scipy import stats as _st\n",
    "from scipy import optimize as _op\n",
    "from lmoments3 import distr as _lmdistr\n",
    "import numpy as _np\n",
    "import matplotlib.pyplot as _plt\n",
    "import numdifftools as _ndt\n",
    "\n",
    "\n",
    "\n",
    "class _Base:\n",
    "    \n",
    "    def __init__(self, data, fit_method = 'mle', \n",
    "                       ci = 0, ci_method = None,\n",
    "                       return_periods = None, frec = 1):        \n",
    "        # Data to be used for the fit\n",
    "        self.data = data\n",
    "        \n",
    "        # Fit method to be used\n",
    "        if fit_method in ['mle', 'mom', 'lmoments']:\n",
    "            self.fit_method = fit_method\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                (\"fit methods accepted are:\\n\"\n",
    "                 \"    mle (Maximum Likelihood Estimation)\\n\"\n",
    "                 \"    lmoments\\n\"\n",
    "                 \"    mom (method of moments)\\n\")\n",
    "            )\n",
    "        \n",
    "        # Calculate shape, location, scale and a frozen distribution\n",
    "        # with the calculated estimators (shape, location, scale)\n",
    "        self._fit()\n",
    "        \n",
    "        # Check for calculations of return periods and return values.\n",
    "        self.frec = frec\n",
    "        if return_periods:\n",
    "            self.return_periods = _np.array(return_periods)\n",
    "            self.return_values = self.distr.isf(self.frec / \n",
    "                                                self.return_periods)\n",
    "        else:\n",
    "            self.return_periods = _np.array([])\n",
    "            self.return_values = _np.array([])\n",
    "        \n",
    "        # Check for the estimation of confidence intervals\n",
    "        if ci  == 0 or 0 < ci < 1:\n",
    "            self.ci = ci\n",
    "        else:\n",
    "            raise ValueError(\"ci should be a value in the interval 0 < ci < 1\")\n",
    "        if self.ci:\n",
    "            if (ci_method and\n",
    "                fit_method == 'mle' and \n",
    "                ci_method in ['delta', 'bootstrap']):\n",
    "                self.ci_method = ci_method\n",
    "                self._ci()\n",
    "            elif (ci_method and\n",
    "                fit_method == 'lmoments' and \n",
    "                ci_method in ['bootstrap']):\n",
    "                self.ci_method = ci_method\n",
    "                self._ci()\n",
    "            elif (ci_method and\n",
    "                fit_method == 'mom' and \n",
    "                ci_method in ['bootstrap']):\n",
    "                self.ci_method = ci_method\n",
    "                self._ci()\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                (\"You should provide a valid value for the confidence\\n\"\n",
    "                 \"interval calculation, 'ci_method'\\n\"))\n",
    "         \n",
    "    \n",
    "    def _fit(self):\n",
    "        # This is a base class and shouldn't be used as it. \n",
    "        # This method should be implemented in the subclass.\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "    \n",
    "    def _ci(self):\n",
    "        # This is a base class and shouldn't be used as it. \n",
    "        # This method should be implemented in the subclass.\n",
    "        raise NotImplementedError(\"Subclasses should implement this!\")\n",
    "    \n",
    "    def pdf(self, quantiles):\n",
    "        # A shortcut to the frozen distribution pdf as provided by scipy.\n",
    "        \"\"\"\n",
    "        Probability density function at x of the given frozen RV.\n",
    "        \n",
    "    \n",
    "        **Parameters**\n",
    "        \n",
    "        x : array_like\n",
    "            quantiles\n",
    "            \n",
    "        **Returns**\n",
    "        \n",
    "        pdf : ndarray\n",
    "            Probability density function evaluated at x\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.distr.pdf(quantiles)\n",
    "    \n",
    "    def cdf(self, quantiles):\n",
    "        # A shortcut to the frozen distribution cdf as provided by scipy.\n",
    "        \"\"\"\n",
    "        Cumulative distribution function of the given frozen RV.\n",
    "        \n",
    "        **Parameters**\n",
    "        \n",
    "        x : array_like\n",
    "            quantiles\n",
    "        **Returns**\n",
    "        \n",
    "        cdf : ndarray\n",
    "            Cumulative distribution function evaluated at `x`\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.distr.cdf(quantiles)\n",
    "        \n",
    "    def ppf(self, q):\n",
    "        # A shortcut to the frozen distribution ppf as provided by scipy.\n",
    "        \"\"\"\n",
    "        Percent point function (inverse of cdf) at q of the given frozen RV.\n",
    "        **Parameters**\n",
    "        \n",
    "        q : array_like\n",
    "            lower tail probability\n",
    "        \n",
    "        **Returns**\n",
    "        \n",
    "        x : array_like\n",
    "            quantile corresponding to the lower tail probability q.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.distr.ppf(q)\n",
    "    \n",
    "    def stats(self, moments):\n",
    "        # A shortcut to the frozen distribution stats as provided by scipy.\n",
    "        \"\"\"\n",
    "        Some statistics of the given RV.\n",
    "        **Parameters**\n",
    "        \n",
    "        moments : str, optional\n",
    "            composed of letters ['mvsk'] defining which moments to compute:\n",
    "            'm' = mean,\n",
    "            'v' = variance,\n",
    "            's' = (Fisher's) skew,\n",
    "            'k' = (Fisher's) kurtosis.\n",
    "            (default='mv')\n",
    "        **Returns**\n",
    "        \n",
    "        stats : sequence\n",
    "            of requested moments.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.distr.stats(moments)    \n",
    "    \n",
    "    def _plot(self, ax, title, xlabel, ylabel):\n",
    "        # helper function for:\n",
    "        #     self.plot_density()\n",
    "        #     self.plot_pp()\n",
    "        #     self.plot_qq()\n",
    "        #     self.plot_return_values()\n",
    "        #     self.plot_summary()\n",
    "       # ax.set_facecolor((0.95, 0.95, 0.95))\n",
    "        _plt.setp(ax.lines, linewidth = 2, color = 'blue')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(xlabel, fontsize = 14)\n",
    "        ax.set_ylabel(ylabel, fontsize = 14)\n",
    "\n",
    "        ax.grid(False)\n",
    "        return ax\n",
    "        \n",
    "    def plot_density(self):\n",
    "        \"\"\"\n",
    "        Histogram of the empirical pdf data and the pdf plot of the \n",
    "        fitted distribution.\n",
    "        All parameters are predefined from the frozen fitted model and empirical\n",
    "        data available.\n",
    "        **Returns**\n",
    "        \n",
    "        Density plot.\n",
    "        \"\"\"\n",
    "        \n",
    "        fig, ax = _plt.subplots(figsize=(8, 6))\n",
    "        \n",
    "        # data\n",
    "        x = _np.linspace(self.distr.ppf(0.001), self.distr.ppf(0.999), 100)\n",
    "        \n",
    "        # plot\n",
    "        ax.plot(x, self.distr.pdf(x), label = 'Fitted', color = 'k')\n",
    "        ax.hist(self.data, range = (0,400),density = True, \n",
    "                color = 'yellow', alpha = 0.75, label = \"Empirical\")\n",
    "        ax = self._plot(ax, ' ', 'x', 'f(x)')\n",
    "        ax.legend(loc='best', frameon=True)     \n",
    "    \n",
    "    def plot_pp(self):\n",
    "        \"\"\"\n",
    "        PP (probability) plot between empirical and fitted data.\n",
    "        All parameters are predefined from the frozen fitted model and empirical\n",
    "        data available.\n",
    "        \n",
    "        **Returns**\n",
    "        \n",
    "        PP plot. \n",
    "        \"\"\"\n",
    "        \n",
    "        fig, ax = _plt.subplots(figsize=(8, 6))\n",
    "        \n",
    "        # data\n",
    "        data = _np.sort(self.data)\n",
    "        N = len(data)\n",
    "        y = _np.arange(1, N + 1) / (N + 1)\n",
    "        x = self.distr.cdf(data)\n",
    "        \n",
    "        # plot\n",
    "        ax.scatter(x, y, color = 'darkcyan')\n",
    "        ax.plot([0, 1], [0, 1])\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax = self._plot(ax, 'P-P Plot', 'model', 'empirical')\n",
    "        \n",
    "    def plot_qq(self):\n",
    "        \"\"\"\n",
    "        QQ (Quantile-Quantile) plot between empirical and fitted data.\n",
    "        All parameters are predefined from the frozen fitted model and empirical\n",
    "        data available.\n",
    "        \n",
    "        **Returns**\n",
    "        \n",
    "        QQ plot. \n",
    "        \"\"\"\n",
    "        \n",
    "        fig, ax = _plt.subplots(figsize=(8, 6))\n",
    "        \n",
    "        # data\n",
    "        y = _np.sort(self.data)\n",
    "        N = len(y)\n",
    "        x = _np.arange(1, N + 1) / (N + 1)\n",
    "        x = self.distr.ppf(x)\n",
    "        \n",
    "        # plot\n",
    "        ax = self._plot(ax, 'Q-Q plot', 'model', 'empirical')\n",
    "        ax.scatter(x, y, color = 'forestgreen')\n",
    "        low_lim = _np.min([x, y]) * 0.95\n",
    "        high_lim = _np.max([x, y]) * 1.05\n",
    "        ax.plot([low_lim, high_lim], [low_lim, high_lim], c='k')\n",
    "        ax.set_xlim(low_lim, high_lim)\n",
    "        ax.set_ylim(low_lim, high_lim)\n",
    "        \n",
    "    def plot_return_values(self):\n",
    "        \"\"\"\n",
    "        Return values and return periods of data. If confidence interval \n",
    "        information has been provided it will show the confidence interval \n",
    "        values.\n",
    "        \n",
    "        **Returns**\n",
    "        \n",
    "        Return values and return periods plot. \n",
    "        \"\"\"\n",
    "        \n",
    "        fig, ax = _plt.subplots(figsize=(8, 6))\n",
    "        \n",
    "        # data\n",
    "        T = _np.arange(0.1, 500.1, 0.1)\n",
    "        sT = self.distr.isf(self.frec * 1./T)\n",
    "        N = _np.r_[1:len(self.data)+1] * self.frec\n",
    "        Nmax = max(N)\n",
    "        \n",
    "        # plot\n",
    "        ax = self._plot(ax, '', 'return period', 'return level') #xXXXXXX sd asd 234234sdfffffffffffffffffffffffffffffffffff \n",
    "        ax.semilogx(T, sT,color='blue')\n",
    "        #ax.plot(T, sT,color='cyan')\n",
    "        ax.scatter(self.frec * Nmax/N, sorted(self.data)[::-1], color = 'magenta')\n",
    "        \n",
    "        # plot confidence intervals if available\n",
    "        if self.ci:\n",
    "            #y1 = sT - st.norm.ppf(1 - self.ci / 2) * np.sqrt(self._ci_se)\n",
    "            #y2 = sT + st.norm.ppf(1 - self.ci / 2) * np.sqrt(self._ci_se)\n",
    "            ax.semilogx(T, self._ci_Td, '--',color='blue')\n",
    "            ax.semilogx(T, self._ci_Tu, '--',color='blue')\n",
    "            #ax.plot(T, self._ci_Td, '--',color='cyan')\n",
    "            #ax.plot(T, self._ci_Tu, '--',color='cyan')\n",
    "            ax.fill_between(T, self._ci_Td, self._ci_Tu, color = '0.75', alpha = 0.5)\n",
    "        \n",
    "    def plot_summary(self):\n",
    "        \"\"\"\n",
    "        Summary plot including PP plot, QQ plot, empirical and fitted pdf and\n",
    "        return values and periods.\n",
    "        \n",
    "        **Returns**\n",
    "        \n",
    "        4-panel plot including PP, QQ, pdf and return level plots\n",
    "        \"\"\"\n",
    "        \n",
    "        fig, ((ax3, ax2), (ax4, ax1)) = _plt.subplots(2, 2, figsize=(8, 6))\n",
    "        \n",
    "        # PDF plot\n",
    "        x = _np.linspace(self.distr.ppf(0.001), \n",
    "                        self.distr.ppf(0.999), \n",
    "                        100)\n",
    "        ax1.plot(x, self.distr.pdf(x), label = 'Fitted')\n",
    "        ax1.hist(self.data, density = True, range = (0,400), \n",
    "                color = 'yellow', alpha = 0.75, label = \"Empirical\",bins='doane')\n",
    "        ax1 = self._plot(ax1, 'Density Plot', 'max flow', 'normalized frequency of events')\n",
    "        ax1.legend(loc='best', frameon=False)\n",
    "        ax1.set_xlim([0,500])\n",
    "        \n",
    "        # QQ plot\n",
    "        data = _np.sort(self.data)\n",
    "        N = len(data)\n",
    "        y = _np.arange(1, N + 1) / (N + 1)\n",
    "        x = self.distr.cdf(data)\n",
    "        ax2.plot([0, 1], [0, 1])\n",
    "        ax2.set_xlim(0, 1)\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2 = self._plot(ax2, 'P-P Plot', 'theoretical quantiles', 'empirical quantiles')\n",
    "        ax2.scatter(x, y, color = 'darkcyan')\n",
    "        \n",
    "        # PP Plot\n",
    "        y = _np.sort(self.data)\n",
    "        N = len(y)\n",
    "        x = _np.arange(1, N + 1) / (N + 1)\n",
    "        x = self.distr.ppf(x)\n",
    "        ax3.scatter(x, y, color = 'forestgreen')\n",
    "        low_lim = _np.min([x, y]) * 0.95\n",
    "        high_lim = _np.max([x, y]) * 1.05\n",
    "        ax3.plot([low_lim, high_lim], [low_lim, high_lim])\n",
    "        ax3.set_xlim(low_lim, high_lim)\n",
    "        ax3.set_ylim(low_lim, high_lim)\n",
    "        ax3 = self._plot(ax3, 'Q-Q Plot', 'theoretical quantiles', 'empirical quantiles')\n",
    "        \n",
    "        # Return levels plot\n",
    "        T = _np.arange(0.1, 500.1, 0.1)\n",
    "        sT = self.distr.isf(self.frec/T)\n",
    "        N = _np.r_[1:len(self.data)+1] * self.frec\n",
    "        Nmax=max(N)\n",
    "        ax4 = self._plot(ax4, 'Return Level Plot', \n",
    "                              'Return period', \n",
    "                              'Return level')\n",
    "        #ax4.semilogx(T, sT, 'k')\n",
    "        ax4.scatter(self.frec * Nmax/N, sorted(self.data)[::-1], color = 'orangered')   #<------\n",
    "        if self.ci:\n",
    "            #y1 = sT - st.norm.ppf(1 - self.ci / 2) * np.sqrt(self._ci_se)\n",
    "            #y2 = sT + st.norm.ppf(1 - self.ci / 2) * np.sqrt(self._ci_se)\n",
    "            ax4.semilogx(T, self._ci_Td, '--')\n",
    "            ax4.semilogx(T, self._ci_Tu, '--')\n",
    "            ax4.fill_between(T, self._ci_Td, self._ci_Tu, color = '0.75', alpha = 0.5)\n",
    "        \n",
    "        # I love matplotlib for stuff like this, thanks, guys!!!\n",
    "        _plt.tight_layout()\n",
    "        return fig, ax1, ax2, ax3, ax4\n",
    "        \n",
    "        \n",
    "class GEV(_Base):\n",
    "    \"\"\"\n",
    "    Class to fit data to a Generalised extreme value (GEV) distribution.\n",
    "    \n",
    "    **Parameters**\n",
    "        \n",
    "    data : array_like\n",
    "        1D array_like with the extreme values to be considered\n",
    "    fit_method : str\n",
    "        String indicating the method used to fit the distribution.\n",
    "        Availalable values are 'mle' (default value), 'mom' and 'lmoments'.\n",
    "    ci : float (optional)\n",
    "        Float indicating the value to be used for the calculation of the \n",
    "        confidence interval. The returned values are (ci/2, 1-ci/2) \n",
    "        percentile confidence intervals. E.g., a value of 0.05 will \n",
    "        return confidence intervals at 0.025 and 0.975 percentiles.\n",
    "    ci_method : str (optional)\n",
    "        String indicating the method to be used to calculate the \n",
    "        confidence intervals. If ``ci`` is not supplied this parameter will \n",
    "        be ignored. Possible values depend of the fit method chosen. If \n",
    "        the fit method is 'mle' possible values for ci_method are \n",
    "        'delta' and 'bootstrap', if the fit method is 'mom' or \n",
    "        'lmoments' possible value for ci_method is 'bootstrap'.\n",
    "            'delta' is for delta method.\n",
    "            'bootstrap' is for parametric bootstrap.\n",
    "    return_period : array_like (optional)\n",
    "        1D array_like of values for the *return period*. Values indicate\n",
    "        **years**. \n",
    "    frec : int or float\n",
    "        Value indicating the frecuency of events per year. If frec is \n",
    "        not provided the data will be treated as yearly data (1 value per \n",
    "        year).\n",
    "    **Attributes and Methods**\n",
    "    \n",
    "    params : OrderedDict\n",
    "        Ordered dictionary with the values of the *shape*, *location* and\n",
    "        *scale* parameters of the distribution.\n",
    "    c : flt\n",
    "        Float value for the *shape* parameter of the distribution.\n",
    "    loc : flt\n",
    "        Float value for the *location* parameter of the distribution.\n",
    "    scale : flt\n",
    "        Float value for the *scale* parameter of the distribution.\n",
    "    distr : object\n",
    "        Frozen RV object with the same methods of a continuous scipy\n",
    "        distribution but holding the given *shape*, *location*, and *scale* \n",
    "        fixed. See http://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "        for more info.\n",
    "    data : array_like\n",
    "        Input data used for the fit\n",
    "    fit_method : str\n",
    "        String indicating the method used to fit the distribution,\n",
    "        values can be 'mle', 'mom' or 'lmoments'.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _fit(self):\n",
    "        # Fit can be made using Maximum Likelihood Estimation (mle) or using\n",
    "        # l-moments. \n",
    "        # L-moments is fast and accurate most of the time for the GEV\n",
    "        # distribution. \n",
    "        \n",
    "        # MLE FIT\n",
    "        # In the case of the mle estimation, sometimes we get unstable values\n",
    "        # if we don't provide an initial guess of the parameters. Loc and scale \n",
    "        # are more or less stable but shape can be quite unstable depending the \n",
    "        # input data. This is why we are using lmoments to obtain start values \n",
    "        # for the mle optimization. For mle we are using fmin_bfgs as it is \n",
    "        # faster than others and with the first guess provide accurate results.\n",
    "        if self.fit_method == 'mle':\n",
    "            # Initial guess to make the fit of GEV more stable\n",
    "            # To do the initial guess we are using lmoments...\n",
    "            _params0 = _lmdistr.gev.lmom_fit(self.data)\n",
    "            # The mle fit will start with the initial estimators obtained\n",
    "            # with lmoments above\n",
    "            _params = _st.genextreme.fit(self.data, _params0['c'],\n",
    "                                        loc = _params0['loc'],\n",
    "                                        scale = _params0['scale'],\n",
    "                                        optimizer = _op.fmin_bfgs)\n",
    "            self.params = OrderedDict()\n",
    "            # For the shape parameter the value provided by scipy \n",
    "            # is defined as negative as that obtained from other\n",
    "            # packages in R, some textbooks, wikipedia,... ¿?\n",
    "            self.params[\"shape\"]    = _params[0]\n",
    "            self.params[\"location\"] = _params[1]\n",
    "            self.params[\"scale\"]    = _params[2]\n",
    "        \n",
    "        # L-MOMENTS FIT\n",
    "        if self.fit_method == 'lmoments':\n",
    "            _params = _lmdistr.gev.lmom_fit(self.data)\n",
    "            self.params = OrderedDict()\n",
    "            # For the shape parameter the value provided by lmoments3 \n",
    "            # is defined as negative as that obtained from other\n",
    "            # packages in R, some textbooks, wikipedia,... ¿?\n",
    "            self.params[\"shape\"]    = _params['c']\n",
    "            self.params[\"location\"] = _params['loc']\n",
    "            self.params[\"scale\"]    = _params['scale']\n",
    "        \n",
    "        # METHOD OF MOMENTS FIT\n",
    "        if self.fit_method == 'mom':\n",
    "            _params = _gev_momfit(self.data)\n",
    "            self.params = OrderedDict()\n",
    "            self.params[\"shape\"]    = _params[0]\n",
    "            self.params[\"location\"] = _params[1]\n",
    "            self.params[\"scale\"]    = _params[2]\n",
    "        \n",
    "        # Estimators and a frozen distribution for the estimators\n",
    "        self.c     = self.params['shape']      # shape\n",
    "        self.loc   = self.params['location']   # location\n",
    "        self.scale = self.params['scale']      # scale\n",
    "        self.distr = _st.genextreme(self.c,     # frozen distribution\n",
    "                                   loc = self.loc, \n",
    "                                   scale = self.scale)\n",
    "                                   \n",
    "    def _nnlf(self, theta):\n",
    "        # This is used to calculate the variance-covariance matrix using the\n",
    "        # Hessian from numdifftools\n",
    "        # see self._ci_delta() method below\n",
    "        \n",
    "        x = self.data\n",
    "        \n",
    "        # Here we provide code for the GEV distribution and for the special\n",
    "        # case when shape parameter is 0 (Gumbel distribution).\n",
    "        if len(theta) == 3:\n",
    "            c = theta[0]\n",
    "            loc = theta[1]\n",
    "            scale = theta[2]\n",
    "        if len(theta) == 2:\n",
    "            c = 0\n",
    "            loc = theta[0]\n",
    "            scale = theta[1]\n",
    "        if c != 0:\n",
    "            expr = 1. + c * ((x - loc) / scale)\n",
    "            return (len(x) * _np.log(scale) + \n",
    "                   (1. + 1. / c) * _np.sum(_np.log(expr)) +\n",
    "                   _np.sum(expr ** (-1. / c)))\n",
    "        else:\n",
    "            expr = (x - loc) / scale\n",
    "            return (len(x) * _np.log(scale) + \n",
    "                   _np.sum(expr) +\n",
    "                   _np.sum(_np.exp(-expr)))\n",
    "     \n",
    "    def _ci_delta(self):\n",
    "        # Calculate the variance-covariance matrix using the\n",
    "        # hessian from numdifftools\n",
    "        # This is used to obtain confidence intervals for the estimators and\n",
    "        # the return values for several return values.\n",
    "        #\n",
    "        # More info about the delta method can be found on:\n",
    "        #     - Coles, Stuart: \"An Introduction to Statistical Modeling of  \n",
    "        #     Extreme Values\", Springer (2001)\n",
    "        #     - https://en.wikipedia.org/wiki/Delta_method\n",
    "        \n",
    "        # data\n",
    "        c  = -self.c    # We negate the shape to avoid inconsistency problems!?\n",
    "        loc = self.loc\n",
    "        scale = self.scale\n",
    "        hess = _ndt.Hessian(self._nnlf)\n",
    "        T = _np.arange(0.1, 500.1, 0.1)\n",
    "        sT = -_np.log(1.-self.frec/T)\n",
    "        sT2 = self.distr.isf(self.frec/T)\n",
    "        \n",
    "        # VarCovar matrix and confidence values for estimators and return values\n",
    "        # Confidence interval for return values (up values and down values)\n",
    "        ci_Tu = _np.zeros(sT.shape)\n",
    "        ci_Td = _np.zeros(sT.shape)\n",
    "        if c:         # If c then we are calculating GEV confidence intervals\n",
    "            varcovar = _np.linalg.inv(hess([c, loc, scale]))\n",
    "            self.params_ci = OrderedDict()\n",
    "            se = _np.sqrt(_np.diag(varcovar))\n",
    "            self._se = se\n",
    "            self.params_ci['shape']    = (self.c - _st.norm.ppf(1 - self.ci / 2) * se[0],\n",
    "                                          self.c + _st.norm.ppf(1 - self.ci / 2) * se[0])\n",
    "            self.params_ci['location'] = (self.loc - _st.norm.ppf(1 - self.ci / 2) * se[1],\n",
    "                                          self.loc + _st.norm.ppf(1 - self.ci / 2) * se[1])\n",
    "            self.params_ci['scale']    = (self.scale - _st.norm.ppf(1 - self.ci / 2) * se[2],\n",
    "                                          self.scale + _st.norm.ppf(1 - self.ci / 2) * se[2])\n",
    "            for i, val in enumerate(sT2):\n",
    "                gradZ = [scale * (c**-2) * (1 - sT[i] ** (-c)) - scale * (c**-1) * (sT[i]**-c) * _np.log(sT[i]),\n",
    "                         1, \n",
    "                         -(1 - sT[i] ** (-c)) / c]\n",
    "                se = _np.dot(_np.dot(gradZ, varcovar), _np.matrix(gradZ).T)\n",
    "                ci_Tu[i] = val + _st.norm.ppf(1 - self.ci / 2) * _np.sqrt(se)\n",
    "                ci_Td[i] = val - _st.norm.ppf(1 - self.ci / 2) * _np.sqrt(se)\n",
    "        else:         # else then we are calculating Gumbel confidence intervals\n",
    "            varcovar = _np.linalg.inv(hess([loc, scale]))\n",
    "            self.params_ci = OrderedDict()\n",
    "            se = _np.sqrt(_np.diag(varcovar))\n",
    "            self._se = se\n",
    "            self.params_ci['shape']    = (0, 0)\n",
    "            self.params_ci['location'] = (self.loc - _st.norm.ppf(1 - self.ci / 2) * se[0],\n",
    "                                          self.loc + _st.norm.ppf(1 - self.ci / 2) * se[0])\n",
    "            self.params_ci['scale']    = (self.scale - _st.norm.ppf(1 - self.ci / 2) * se[1],\n",
    "                                          self.scale + _st.norm.ppf(1 - self.ci / 2) * se[1])\n",
    "            for i, val in enumerate(sT2):\n",
    "                gradZ = [1, -_np.log(sT[i])]\n",
    "                se = _np.dot(_np.dot(gradZ, varcovar), _np.matrix(gradZ).T)\n",
    "                ci_Tu[i] = val + _st.norm.ppf(1 - self.ci / 2) * _np.sqrt(se)\n",
    "                ci_Td[i] = val - _st.norm.ppf(1 - self.ci / 2) * _np.sqrt(se)\n",
    "        self._ci_Tu = ci_Tu\n",
    "        self._ci_Td = ci_Td\n",
    "    \n",
    "    def _ci_bootstrap(self):\n",
    "        # Calculate confidence intervals using parametric bootstrap and the\n",
    "        # percentil interval method\n",
    "        # This is used to obtain confidence intervals for the estimators and\n",
    "        # the return values for several return values.\n",
    "        # all the code in skextremes.utils.bootstrap_ci has been adapted and\n",
    "        # simplified from that on https://github.com/cgevans/scikits-bootstrap.\n",
    "        #\n",
    "        # More info about bootstrapping can be found on:\n",
    "        #     - https://github.com/cgevans/scikits-bootstrap\n",
    "        #     - Efron: \"An Introduction to the Bootstrap\", Chapman & Hall (1993)\n",
    "        #     - https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29\n",
    "        \n",
    "        # parametric bootstrap for return levels and parameters   \n",
    "        \n",
    "        # The function to bootstrap     \n",
    "        def func(data):\n",
    "            sample = _st.genextreme.rvs(self.c, \n",
    "                                       loc = self.loc, \n",
    "                                       scale = self.scale, \n",
    "                                       size = len(self.data))\n",
    "            c, loc, scale = _st.genextreme.fit(sample, self.c, \n",
    "                                              loc = self.loc, \n",
    "                                              scale = self.scale,\n",
    "                                              optimizer = _op.fmin_bfgs)\n",
    "            T = _np.arange(0.1, 500.1, 0.1)\n",
    "            sT = _st.genextreme.isf(self.frec/T, c, loc = loc, scale = scale)\n",
    "            res = [c, loc, scale]\n",
    "            res.extend(sT.tolist())\n",
    "            return tuple(res)\n",
    "        \n",
    "        # the calculations itself\n",
    "        out = _bsci(self.data, statfunction = func, n_samples = 500)\n",
    "        self._ci_Td = out[0, 3:]\n",
    "        self._ci_Tu = out[1, 3:]\n",
    "        self.params_ci = OrderedDict()\n",
    "        self.params_ci['shape']    = (out[0,0], out[1,0])\n",
    "        self.params_ci['location'] = (out[0,1], out[1,1])\n",
    "        self.params_ci['scale']    = (out[0,2], out[1,3])\n",
    "    \n",
    "    def _ci(self):\n",
    "        # Method called internally to calculate confidence intervals if \n",
    "        # required. To see more info about available methods see comments on\n",
    "        # self._ci_delta and self._ci_bootstrap methods.\n",
    "        \n",
    "        if self.ci_method == \"delta\":\n",
    "            self._ci_delta()\n",
    "        if self.ci_method == \"bootstrap\":\n",
    "            self._ci_bootstrap()\n",
    "\n",
    "class Gumbel(GEV):\n",
    "    __doc__ = GEV.__doc__.replace(\"Generalised extreme value (GEV) distribution.\", \n",
    "                                  (\"Gumbel distribution. Note that this is a \"\n",
    "                                   \"special case of the ``GEV`` class where \"\n",
    "                                   \"the 'shape' is fixed to 0.\"))\n",
    "    \n",
    "    def _fit(self):\n",
    "        \n",
    "        if self.fit_method == 'mle':\n",
    "            _params = _st.gumbel_r.fit(self.data)\n",
    "            self.params = OrderedDict()\n",
    "            self.params[\"shape\"]    = 0\n",
    "            self.params[\"location\"] = _params[0]\n",
    "            self.params[\"scale\"]    = _params[1]\n",
    "        \n",
    "        if self.fit_method == 'lmoments':\n",
    "            _params = _lmdistr.gum.lmom_fit(self.data)\n",
    "            self.params = OrderedDict()\n",
    "            self.params[\"shape\"]    = 0\n",
    "            self.params[\"location\"] = _params['loc']\n",
    "            self.params[\"scale\"]    = _params['scale']\n",
    "        \n",
    "        # METHOD OF MOMENTS FIT\n",
    "        if self.fit_method == 'mom':\n",
    "            _params = _gum_momfit(self.data)\n",
    "            self.params = OrderedDict()\n",
    "            self.params[\"shape\"]    = _params[0]\n",
    "            self.params[\"location\"] = _params[1]\n",
    "            self.params[\"scale\"]    = _params[2]\n",
    "        \n",
    "        self.c     = self.params['shape']\n",
    "        self.loc   = self.params['location']\n",
    "        self.scale = self.params['scale']\n",
    "        self.distr = _st.gumbel_r(loc = self.loc, \n",
    "                                   scale = self.scale)\n",
    "\n",
    "class GPD(_Base):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
